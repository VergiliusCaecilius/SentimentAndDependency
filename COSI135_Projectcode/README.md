# Project Evaluation

## Scope 
The scope of this project was to see whether weighting word vectors by a scalar based upon their depdency relationship within the sentences of the training data might be useful for text categorization of positive and negative sentiment. The linguistic theory behind this is that it is often said that verbs are the core of the sentence and the rest of the sentence is built around them. As such, it might be reasonable to think that weighting the verb as more important will increase the accuracy of the model as compared to a Bag of Words approach. EG "I love you"  and "I hate you" are almost entirely different but "I hate this" and "I hate that" are rather similar.

### Key Points to Include:
- **Project Objective**: To explore the effects of weighting words vectors based on their dependency for sentiment classification. 
- **What's the deliverable?**: A model in which you can initially weight word vectors on the traing-data to artificially raise or lower the magnitude of certain word vectors and hence their importance in the model. Along with thise a nearly identical model that doesn't weight the vectors to use as a neutral baseline.
- **Boundaries**: We do not deal much beyond one sentneces reviews and if this weighting model might be more useful for non theme based classification such as identifying a literary genre given a text. We also did not take a look beyond binary labels such positive, negative, neutral classification. Nor did we implement multi-label where a text can be labeled as more than one category. This is because something cannot be both positive and negative.
- **pipeline** imagine you are a peice of training data. The first thing that happens to you is that you are read into a tuple that is in a list and your annotation of 1 or 0 is turned into a dictionary format. Then you are processed into a doc a datatype. Here you split into a goldlabel version and non-gold label version. These two versions are then put back together into an example datatype. You are then fed to the train function as part a batch. 

## Implementation 
- **Technical Approach**: We decided to add a textcat pipeline to spacy which classifies an input based on the number of categories/labels you give to pipeline and annotations you add to your docs(a doc is Spacey datatype). We then need to prepare our annotated docs to be fed into the model. We do this by creating an Example datatype which holds one doc with the gold label and one with the initial training label. The model then trains on the Examples using the update function within the train function of our code. 

- **Workflow**: Within this folder you will see three files: BaseLineModel, dataCleaning, and weights_model. 
    - **dataCleaning:** We will start with dataCleaning for this is the chronological place start. This file has the code that takes our raw Kaggle data formats it and then splits into a textfile of traing, dev, and test set split 70% training, 15 %dev, 15% test, respectively. 
    - **baseline:**  Within the baseline model folder you will find the data which we have prepared and split into a training, dev, and test set. We then load the spacey 
    - **Weights_model:** The weights model is by design fundamentally very similar to the baseline mode. In fact, it was built of off it. It contains the same the dev, test, and training set and the code is mostly identical. However, it contains two new functions extract_dependency_layers and weight_sentnces.
    
- **MORE**: *for more info on the meat and potatoes of implementation look at Function Overview below*


## Execution and How to Run 
You can see the effects of the weighted model or baselined model by running weightModel.py or newBaseModel.py. To see the results over multiple hyperparameters, you can look at the csv file generated by the weighted model or you can more data by going into read_score.py file asssociated with each Model. A few hyperparameters were set to a constant because of computational time limitations. If you are reasobly nervous by my decision to use .pkl files you can comment out those lines of code.


## Function Overview 
- **process_kaggle** (file_path: str) -> List[Tuple[str, dict[str, dict[str, float]]]]
    - this function takes in our clean data and returns the positive negative annotation of the review formated as a dictionary where POSITIVE and NEGATIVE are keys and the value of 1 indicates whether it is positive or negative.


- **extract_dependency_layers**(sentences: str) -> List[List[List[str]]]:
    - This function takes in a sentence and returns the words of a sentnece within a List of List of Lists. The most outer List is to handle the case of "When I went, I died" SpaCy treasts "when I went" and "i died" as two seperate layers. The from those two sub sentences the main verb and it's dependencies etc.. It's written recursively because we are dong the same task to many different words. Find it's dependency put it in a list. Find those dependencies put it in a list.

- **weight_sentences**(datalayer, k):
    - This function is designed to take in the result of _extract_dependency_layers. It then goes through each layer of dependencies in the sentences and give multiplies by them by k or a fraction of k. The main verb of a sentences always gets k. Then the dependencies of the main verb get scaled by k/2. Then the depdency of those get sclaed by k/3 etc.


- **make_model_data**(data: tuple[str, dict[str, dict[str, float]]]) -> Iterable[example ]
    - This formats our data and turns our annotated text into the doc datatype. It then turns that doc datatype into an Example datatype which we can feed into the train fucntion. Each example datatype is made up of two docs. One with the gold label annotation, and one with which the model can update it's guess on.


- **train**(train_example, a_batch_size, an_epoch, a_dropnum, a_learnrate):
    - This uses the update function and trains the model. It print's out the loss for the doc object categories.

- **sweep()**:
    - This function goes through and changes some hyperparameters of the train function to see how much they effect the models results. It stores these results as dictionary which is pickled and then passed of to the read_score.py script. 
 
